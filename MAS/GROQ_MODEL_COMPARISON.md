# Groq Model Comparison

## General Information

This document states the comparison and analysis of the scaffolding responses generated by accessible models that were provided by the Groq platform.

## Generation Of Comparison Files

1. Agent responses were generated and saved using the `MAS\tests\model_comparacement.py` script. 
2. For the proper testing conditions all models were prompted with the same concept map data and agent sequence. 
3. All models were tested for the native support of the AI client interace class `AIManager` used in the scaffolding project. In case when the model fails to produce the response due to inability to process the provided parametes (eg. temperature), it will be disqualified from the comparacement.
4. Data collected during the response generation includes the collective response time and the amount of tokens used in the message generation. These traits were minded during the comparacement process alongside with the quality of the agent's message.

## Comparison Candidates

Following models were quered for the comparacement:

- "deepseek-r1-distill-llama-70b"
- "gemma2-9b-it"
- "llama-3.1-8b-instant"
- "llama-3.3-70b-versatile"
- "compound-beta"
- "openai/gpt-oss-120b"
- "qwen/qwen3-32b"

## Comparison Results

### Generation Successfulness

None of the candidates have failed to generate a response though the OpenAI interace using the provided parameters.

### Response Quality 

The model responses were analyzed by following criterion:

1. **Hepfulness**: determines whether the message contains enough of useful information for the user to expand their concept map based on the response. When the direct suggestions are not provied, it's also important to check whether the conversation with the agent can be logically continued from the responses.
2. **Reasonability**: determines whether the response stays on the topic of the discussion. The agent should make reasonable conclusions using the contents of the concept maps. The model should act within the requested agent role boundaries.
3. **Clarity**: determines the fluency of the conversation. Good responses should be short, but meaningful. The usage of complex phrasing should be minimized.


**Best overall balance of depth, clarity, and scaffolding**

1. llama-3.3-70b-versatile

   * Pros: Very consistent, nuanced (mentions both cooling and warming volcanic effects), suggests feedback loops, encourages exploration of industries and human activities.
   * Messages are supportive, precise, and pedagogically aligned.
   * **This model is most suitable as the “teaching agent".**

2. openai/gpt-oss-120b

   * Pros: Clear and well-structured, introduces intermediate steps (e.g., gases/ash), prompts user reflection, and integrates unused concepts (industries, deforestation) very smoothly.
   * Balanced between encouragement and guidance.

---

**Helpful, but weak responses**

3. deepseek-r1-distill-llama-70b

   * Pros: Provides thoughtful “inner reasoning” (via <think>), scaffolds with clear questions, encourages corrections to misconceptions.
   * Cons: Sometimes verbose, reasoning text might “leak” into the answer (less clean for user-facing scaffolding).

Overall: Strong reasoning, but not as polished for direct learner interaction.

4. qwen/qwen3-32b

   * Pros: Offers nuanced prompts (mentions aerosols vs greenhouse gases, reciprocal links), checks accuracy, encourages reflection.
   * Cons: Slightly more technical and wordy; may risk confusing younger/less prepared learners.

Overall: Solid, but needs simplification.

---

**Oversimplified or generic responses**

5. compound-beta

   * Pros: Supportive, logical progression, suggests feedback loops and missing connections.
   * Cons: Less detailed than top-tier models, sometimes generic (e.g., “what other factors could relate?” without specifics).
   * **Decent fallback model.**

6. llama-3.1-8b-instant

   * Pros: Clear and encouraging, does point out missing connections.
   * Cons: More generic, less nuanced than 70B counterpart, doesn’t challenge misconceptions much.

Overall: Usable for lightweight tasks but not ideal for deeper scaffolding.

---

**Too shallow/simplistic responses**

7. gemma2-9b-it

   * Pros: Friendly, concise, easy to understand.
   * Cons: Far too generic (“That’s a good start! What else could contribute?”), rarely adds real conceptual depth or correction of misconceptions.

Overall: Simplest responses, not suitable for nuanced scaffolding.


### Efficiency Ranking

Response time and token usage were taken into consideration during the comparison. The results of data collection are following:

* gemma2-9b-it            → 1.8s,  1420 tokens
* llama-3.1-8b-instant    → 3.3s,  1669 tokens
* openai/gpt-oss-120b     → 3.3s,  2113 tokens
* llama-3.3-70b-versatile → 3.35s, 1734 tokens
* compound-beta           → 4.8s,  8818 tokens
* deepseek-r1-distill-llama-70b →  8.3s, 2751 tokens
* qwen/qwen3-32b          → 9.6s,  3103 tokens

## Comparison Conclusion 

The model **llama-3.3-70b-versatile** provided by the Groq platform is considered the best overall choise for the experiment requirements. The response generation time is short enough to keep an active and meaningful and overall consistent conversation between the agents and the user. Model's messages are supportive and precise.
